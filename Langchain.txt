ChatGoogleGenerativeAI
class langchain_google_genai.chat_models.ChatGoogleGenerativeAI
[source]

Bases: _BaseGoogleGenerativeAI, BaseChatModel

Google AI chat models integration.

    Instantiation:

        To use, you must have either:

                The GOOGLE_API_KEY` environment variable set with your API key, or

                Pass your API key using the google_api_key kwarg to the ChatGoogle constructor.

        from langchain_google_genai import ChatGoogleGenerativeAI

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro")
        llm.invoke("Write me a ballad about LangChain")

Invoke:

    messages = [
        ("system", "Translate the user sentence to French."),
        ("human", "I love programming."),
    ]
    llm.invoke(messages)

AIMessage(
    content="J'adore programmer. \n",
    response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]},
    id='run-56cecc34-2e54-4b52-a974-337e47008ad2-0',
    usage_metadata={'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23}
)

Stream:

    for chunk in llm.stream(messages):
        print(chunk)

AIMessageChunk(content='J', response_metadata={'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e905f4f4-58cb-4a10-a960-448a2bb649e3', usage_metadata={'input_tokens': 18, 'output_tokens': 1, 'total_tokens': 19})
AIMessageChunk(content="'adore programmer.

â€œ, response_metadata={â€˜finish_reasonâ€™: â€˜STOPâ€™, â€˜safety_ratingsâ€™: [{â€˜categoryâ€™: â€˜HARM_CATEGORY_SEXUALLY_EXPLICITâ€™, â€˜probabilityâ€™: â€˜NEGLIGIBLEâ€™, â€˜blockedâ€™: False}, {â€˜categoryâ€™: â€˜HARM_CATEGORY_HATE_SPEECHâ€™, â€˜probabilityâ€™: â€˜NEGLIGIBLEâ€™, â€˜blockedâ€™: False}, {â€˜categoryâ€™: â€˜HARM_CATEGORY_HARASSMENTâ€™, â€˜probabilityâ€™: â€˜NEGLIGIBLEâ€™, â€˜blockedâ€™: False}, {â€˜categoryâ€™: â€˜HARM_CATEGORY_DANGEROUS_CONTENTâ€™, â€˜probabilityâ€™: â€˜NEGLIGIBLEâ€™, â€˜blockedâ€™: False}]}, id=â€™run-e905f4f4-58cb-4a10-a960-448a2bb649e3â€™, usage_metadata={â€˜input_tokensâ€™: 18, â€˜output_tokensâ€™: 5, â€˜total_tokensâ€™: 23})

        stream = llm.stream(messages)
        full = next(stream)
        for chunk in stream:
            full += chunk
        full

AIMessageChunk(
    content="J'adore programmer. \n",
    response_metadata={'finish_reason': 'STOPSTOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]},
    id='run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec',
    usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}
)

Async:

    await llm.ainvoke(messages)

    # stream:
    # async for chunk in (await llm.astream(messages))

    # batch:
    # await llm.abatch([messages])

Tool calling:

    from pydantic import BaseModel, Field


    class GetWeather(BaseModel):
        '''Get the current weather in a given location'''

        location: str = Field(
            ..., description="The city and state, e.g. San Francisco, CA"
        )


    class GetPopulation(BaseModel):
        '''Get the current population in a given location'''

        location: str = Field(
            ..., description="The city and state, e.g. San Francisco, CA"
        )


    llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])
    ai_msg = llm_with_tools.invoke(
        "Which city is hotter today and which is bigger: LA or NY?"
    )
    ai_msg.tool_calls

[{'name': 'GetWeather',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'c186c99f-f137-4d52-947f-9e3deabba6f6'},
 {'name': 'GetWeather',
  'args': {'location': 'New York City, NY'},
  'id': 'cebd4a5d-e800-4fa5-babd-4aa286af4f31'},
 {'name': 'GetPopulation',
  'args': {'location': 'Los Angeles, CA'},
  'id': '4f92d897-f5e4-4d34-a3bc-93062c92591e'},
 {'name': 'GetPopulation',
  'args': {'location': 'New York City, NY'},
  'id': '634582de-5186-4e4b-968b-f192f0a93678'}]

Structured output:

    from typing import Optional

    from pydantic import BaseModel, Field


    class Joke(BaseModel):
        '''Joke to tell user.'''

        setup: str = Field(description="The setup of the joke")
        punchline: str = Field(description="The punchline to the joke")
        rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")


    structured_llm = llm.with_structured_output(Joke)
    structured_llm.invoke("Tell me a joke about cats")

Joke(
    setup='Why are cats so good at video games?',
    punchline='They have nine lives on the internet',
    rating=None
)

Image input:

    import base64
    import httpx
    from langchain_core.messages import HumanMessage

    image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
    image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")
    message = HumanMessage(
        content=[
            {"type": "text", "text": "describe the weather in this image"},
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
            },
        ]
    )
    ai_msg = llm.invoke([message])
    ai_msg.content

'The weather in this image appears to be sunny and pleasant. The sky is a bright blue with scattered white clouds, suggesting fair weather. The lush green grass and trees indicate a warm and possibly slightly breezy day. There are no signs of rain or storms.

â€˜

    Token usage:

        ai_msg = llm.invoke(messages)
        ai_msg.usage_metadata

{'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23}

Response metadata .. code-block:: python

    ai_msg = llm.invoke(messages) ai_msg.response_metadata

{
    'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},
    'finish_reason': 'STOP',
    'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]
}

Note

ChatGoogleGenerativeAI implements the standard Runnable Interface. ðŸƒ

The Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.

param additional_headers: Dict[str, str] | None = None

    A key-value dictionary representing additional headers for the model call

param cache: BaseCache | bool | None = None

    Whether to cache the response.

        If true, will use the global cache.

        If false, will not use a cache

        If None, will use the global cache if itâ€™s set, otherwise no cache.

        If instance of BaseCache, will use the provided cache.

    Caching is not currently supported for streaming methods of models.

param callback_manager: BaseCallbackManager | None = None

    Deprecated since version 0.1.7: Use callbacks() instead.

    Callback manager to add to the run trace.

param callbacks: Callbacks = None

    Callbacks to add to the run trace.

param client_options: Dict | None = None

    A dictionary of client options to pass to the Google API client, such as api_endpoint.

param convert_system_message_to_human: bool = False

    Whether to merge any leading SystemMessage into the following HumanMessage.

    Gemini does not support system messages; any unsupported messages will raise an error.

param credentials: Any = None

    The default custom credentials (google.auth.credentials.Credentials) to use

param custom_get_token_ids: Callable[[str], list[int]] | None = None

    Optional encoder to use for counting tokens.

param disable_streaming: bool | Literal['tool_calling'] = False

    Whether to disable streaming for this model.

    If streaming is bypassed, then stream()/astream() will defer to invoke()/ainvoke().

        If True, will always bypass streaming case.

        If â€œtool_callingâ€, will bypass streaming case only when the model is called with a tools keyword argument.

        If False (default), will always use streaming case if available.

param google_api_key: SecretStr | None [Optional] (alias 'api_key')

    Google AI API key. If not specified will be read from env var GOOGLE_API_KEY.

param max_output_tokens: int | None = None (alias 'max_tokens')

    Maximum number of tokens to include in a candidate. Must be greater than zero. If unset, will default to 64.

param max_retries: int = 6

    The maximum number of retries to make when generating.

param metadata: dict[str, Any] | None = None

    Metadata to add to the run trace.

param model: str [Required]

    Model name to use.

    The name of the model to use. Supported examples:

            gemini-pro

            models/text-bison-001

param n: int = 1

    Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.

param rate_limiter: BaseRateLimiter | None = None

    An optional rate limiter to use for limiting the number of requests.

param safety_settings: Dict[HarmCategory, HarmBlockThreshold] | None = None

    The default safety settings to use for all generations.

    For example:

        from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory

        safety_settings = {

            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,

        }

param tags: list[str] | None = None

    Tags to add to the run trace.

param temperature: float = 0.7

    Run inference with this temperature. Must by in the closed interval [0.0, 1.0].

param timeout: float | None = None

    The maximum number of seconds to wait for a response.

param top_k: int | None = None

    Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.

param top_p: float | None = None

    Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].

param transport: str | None = None

    A string, one of: [rest, grpc, grpc_asyncio].

param verbose: bool [Optional]

    Whether to print out response text.

__call__(messages: list[BaseMessage], stop: list[str] | None = None, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None, **kwargs: Any) â†’ BaseMessage

    Deprecated since version langchain-core==0.1.7: Use invoke() instead.

    Parameters:

            messages (list[BaseMessage])

            stop (list[str] | None)

            callbacks (list[BaseCallbackHandler] | BaseCallbackManager | None)

            kwargs (Any)

    Return type:

        BaseMessage

async abatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]

    Default implementation runs ainvoke in parallel using asyncio.gather.

    The default implementation of batch works well for IO bound runnables.

    Subclasses should override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode.

    Parameters:

            inputs (list[Input]) â€“ A list of inputs to the Runnable.

            config (RunnableConfig | list[RunnableConfig] | None) â€“ A config to use when invoking the Runnable. The config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing purposes, â€˜max_concurrencyâ€™ for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None.

            return_exceptions (bool) â€“ Whether to return exceptions instead of raising them. Defaults to False.

            kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.

    Returns:

        A list of outputs from the Runnable.
    Return type:

        list[Output]

async abatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ AsyncIterator[tuple[int, Output | Exception]]

    Run ainvoke in parallel on a list of inputs, yielding results as they complete.

    Parameters:

            inputs (Sequence[Input]) â€“ A list of inputs to the Runnable.

            config (RunnableConfig | Sequence[RunnableConfig] | None) â€“ A config to use when invoking the Runnable. The config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing purposes, â€˜max_concurrencyâ€™ for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Defaults to None. Defaults to None.

            return_exceptions (bool) â€“ Whether to return exceptions instead of raising them. Defaults to False.

            kwargs (Any | None) â€“ Additional keyword arguments to pass to the Runnable.

    Yields:

        A tuple of the index of the input and the output from the Runnable.
    Return type:

        AsyncIterator[tuple[int, Output | Exception]]

async ainvoke(input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any) â†’ BaseMessage

    Default implementation of ainvoke, calls invoke from a thread.

    The default implementation allows usage of async code even if the Runnable did not implement a native async version of invoke.

    Subclasses should override this method if they can run asynchronously.

    Parameters:

            input (LanguageModelInput)

            config (Optional[RunnableConfig])

            stop (Optional[list[str]])

            kwargs (Any)

    Return type:

        BaseMessage

async astream(input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any) â†’ AsyncIterator[BaseMessageChunk]

    Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output.

    Parameters:

            input (LanguageModelInput) â€“ The input to the Runnable.

            config (Optional[RunnableConfig]) â€“ The config to use for the Runnable. Defaults to None.

            kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.

            stop (Optional[list[str]])

    Yields:

        The output of the Runnable.
    Return type:

        AsyncIterator[BaseMessageChunk]

async astream_events(input: Any, config: RunnableConfig | None = None, *, version: Literal['v1', 'v2'], include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any) â†’ AsyncIterator[StandardStreamEvent | CustomStreamEvent]

Generate a stream of events.

Use to create an iterator over StreamEvents that provide real-time information about the progress of the Runnable, including StreamEvents from intermediate results.

A StreamEvent is a dictionary with the following schema:

    event: str - Event names are of the

        format: on_[runnable_type]_(start|stream|end).

    name: str - The name of the Runnable that generated the event.

    run_id: str - randomly generated ID associated with the given execution of

        the Runnable that emitted the event. A child Runnable that gets invoked as part of the execution of a parent Runnable is assigned its own unique ID.

    parent_ids: List[str] - The IDs of the parent runnables that

        generated the event. The root Runnable will have an empty list. The order of the parent IDs is from the root to the immediate parent. Only available for v2 version of the API. The v1 version of the API will return an empty list.

    tags: Optional[List[str]] - The tags of the Runnable that generated

        the event.

    metadata: Optional[Dict[str, Any]] - The metadata of the Runnable

        that generated the event.

    data: Dict[str, Any]

Below is a table that illustrates some events that might be emitted by various chains. Metadata fields have been omitted from the table for brevity. Chain definitions have been included after the table.

ATTENTION This reference table is for the V2 version of the schema.

event
	

name
	

chunk
	

input
	

output

on_chat_model_start
	

[model name]
		

{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}
	

on_chat_model_stream
	

[model name]
	

AIMessageChunk(content=â€helloâ€)
		

on_chat_model_end
	

[model name]
		

{â€œmessagesâ€: [[SystemMessage, HumanMessage]]}
	

AIMessageChunk(content=â€hello worldâ€)

on_llm_start
	

[model name]
		

{â€˜inputâ€™: â€˜helloâ€™}
	

on_llm_stream
	

[model name]
	

â€˜Helloâ€™
		

on_llm_end
	

[model name]
		

â€˜Hello human!â€™
	

on_chain_start
	

format_docs
			

on_chain_stream
	

format_docs
	

â€œhello world!, goodbye world!â€
		

on_chain_end
	

format_docs
		

[Document(â€¦)]
	

â€œhello world!, goodbye world!â€

on_tool_start
	

some_tool
		

{â€œxâ€: 1, â€œyâ€: â€œ2â€}
	

on_tool_end
	

some_tool
			

{â€œxâ€: 1, â€œyâ€: â€œ2â€}

on_retriever_start
	

[retriever name]
		

{â€œqueryâ€: â€œhelloâ€}
	

on_retriever_end
	

[retriever name]
		

{â€œqueryâ€: â€œhelloâ€}
	

[Document(â€¦), ..]

on_prompt_start
	

[template_name]
		

{â€œquestionâ€: â€œhelloâ€}
	

on_prompt_end
	

[template_name]
		

{â€œquestionâ€: â€œhelloâ€}
	

ChatPromptValue(messages: [SystemMessage, â€¦])

In addition to the standard events, users can also dispatch custom events (see example below).

Custom events will be only be surfaced with in the v2 version of the API!

A custom event has following format:

Attribute
	

Type
	

Description

name
	

str
	

A user defined name for the event.

data
	

Any
	

The data associated with the event. This can be anything, though we suggest making it JSON serializable.

Here are declarations associated with the standard events shown above:

format_docs:

def format_docs(docs: List[Document]) -> str:
    '''Format the docs.'''
    return ", ".join([doc.page_content for doc in docs])

format_docs = RunnableLambda(format_docs)

some_tool:

@tool
def some_tool(x: int, y: str) -> dict:
    '''Some_tool.'''
    return {"x": x, "y": y}

prompt:

template = ChatPromptTemplate.from_messages(
    [("system", "You are Cat Agent 007"), ("human", "{question}")]
).with_config({"run_name": "my_template", "tags": ["my_template"]})

Example:

from langchain_core.runnables import RunnableLambda

async def reverse(s: str) -> str:
    return s[::-1]

chain = RunnableLambda(func=reverse)

events = [
    event async for event in chain.astream_events("hello", version="v2")
]

# will produce the following events (run_id, and parent_ids
# has been omitted for brevity):
[
    {
        "data": {"input": "hello"},
        "event": "on_chain_start",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
    {
        "data": {"chunk": "olleh"},
        "event": "on_chain_stream",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
    {
        "data": {"output": "olleh"},
        "event": "on_chain_end",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
]

Example: Dispatch Custom Event

from langchain_core.callbacks.manager import (
    adispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda, RunnableConfig
import asyncio


async def slow_thing(some_input: str, config: RunnableConfig) -> str:
    """Do something that takes a long time."""
    await asyncio.sleep(1) # Placeholder for some slow operation
    await adispatch_custom_event(
        "progress_event",
        {"message": "Finished step 1 of 3"},
        config=config # Must be included for python < 3.10
    )
    await asyncio.sleep(1) # Placeholder for some slow operation
    await adispatch_custom_event(
        "progress_event",
        {"message": "Finished step 2 of 3"},
        config=config # Must be included for python < 3.10
    )
    await asyncio.sleep(1) # Placeholder for some slow operation
    return "Done"

slow_thing = RunnableLambda(slow_thing)

async for event in slow_thing.astream_events("some_input", version="v2"):
    print(event)

    Parameters:

            input (Any) â€“ The input to the Runnable.

            config (RunnableConfig | None) â€“ The config to use for the Runnable.

            version (Literal['v1', 'v2']) â€“ The version of the schema to use either v2 or v1. Users should use v2. v1 is for backwards compatibility and will be deprecated in 0.4.0. No default will be assigned until the API is stabilized. custom events will only be surfaced in v2.

            include_names (Sequence[str] | None) â€“ Only include events from runnables with matching names.

            include_types (Sequence[str] | None) â€“ Only include events from runnables with matching types.

            include_tags (Sequence[str] | None) â€“ Only include events from runnables with matching tags.

            exclude_names (Sequence[str] | None) â€“ Exclude events from runnables with matching names.

            exclude_types (Sequence[str] | None) â€“ Exclude events from runnables with matching types.

            exclude_tags (Sequence[str] | None) â€“ Exclude events from runnables with matching tags.

            kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable. These will be passed to astream_log as this implementation of astream_events is built on top of astream_log.

    Yields:

        An async stream of StreamEvents.
    Raises:

        NotImplementedError â€“ If the version is not v1 or v2.
    Return type:

        AsyncIterator[StandardStreamEvent | CustomStreamEvent]

batch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ list[Output]

    Default implementation runs invoke in parallel using a thread pool executor.

    The default implementation of batch works well for IO bound runnables.

    Subclasses should override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode.

    Parameters:

            inputs (list[Input])

            config (RunnableConfig | list[RunnableConfig] | None)

            return_exceptions (bool)

            kwargs (Any | None)

    Return type:

        list[Output]

batch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) â†’ Iterator[tuple[int, Output | Exception]]

    Run invoke in parallel on a list of inputs, yielding results as they complete.

    Parameters:

            inputs (Sequence[Input])

            config (RunnableConfig | Sequence[RunnableConfig] | None)

            return_exceptions (bool)

            kwargs (Any | None)

    Return type:

        Iterator[tuple[int, Output | Exception]]

bind(**kwargs: Any) â†’ Runnable[Input, Output]

Bind arguments to a Runnable, returning a new Runnable.

Useful when a Runnable in a chain requires an argument that is not in the output of the previous Runnable or included in the user input.

Parameters:

    kwargs (Any) â€“ The arguments to bind to the Runnable.
Returns:

    A new Runnable with the arguments bound.
Return type:

    Runnable[Input, Output]

Example:

from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser

llm = ChatOllama(model='llama2')

# Without bind.
chain = (
    llm
    | StrOutputParser()
)

chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
# Output is 'One two three four five.'

# With bind.
chain = (
    llm.bind(stop=["three"])
    | StrOutputParser()
)

chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
# Output is 'One two'

bind_tools(tools: Sequence[ToolDict | Tool], tool_config: Dict | _ToolConfigDict | None = None, *, tool_choice: dict | List[str] | str | Literal['auto', 'none', 'any'] | Literal[True] | bool | None = None, **kwargs: Any) â†’ Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage]
[source]

    Bind tool-like objects to this chat model.

    Assumes model is compatible with google-generativeAI tool-calling API.

    Parameters:

            tools (Sequence[ToolDict | Tool]) â€“ A list of tool definitions to bind to this chat model. Can be a pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation.

            **kwargs (Any) â€“ Any additional parameters to pass to the Runnable constructor.

            tool_config (Dict | _ToolConfigDict | None)

            tool_choice (dict | List[str] | str | Literal['auto', 'none', 'any'] | ~typing.Literal[True] | bool | None)

            **kwargs

    Return type:

        Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage]

configurable_alternatives(which: ConfigurableField, *, default_key: str = 'default', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â†’ RunnableSerializable

Configure alternatives for Runnables that can be set at runtime.

Parameters:

        which (ConfigurableField) â€“ The ConfigurableField instance that will be used to select the alternative.

        default_key (str) â€“ The default key to use if no alternative is selected. Defaults to â€œdefaultâ€.

        prefix_keys (bool) â€“ Whether to prefix the keys with the ConfigurableField id. Defaults to False.

        **kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) â€“ A dictionary of keys to Runnable instances or callables that return Runnable instances.

Returns:

    A new Runnable with the alternatives configured.
Return type:

    RunnableSerializable

from langchain_anthropic import ChatAnthropic
from langchain_core.runnables.utils import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatAnthropic(
    model_name="claude-3-sonnet-20240229"
).configurable_alternatives(
    ConfigurableField(id="llm"),
    default_key="anthropic",
    openai=ChatOpenAI()
)

# uses the default model ChatAnthropic
print(model.invoke("which organization created you?").content)

# uses ChatOpenAI
print(
    model.with_config(
        configurable={"llm": "openai"}
    ).invoke("which organization created you?").content
)

configurable_fields(**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â†’ RunnableSerializable

Configure particular Runnable fields at runtime.

Parameters:

    **kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) â€“ A dictionary of ConfigurableField instances to configure.
Returns:

    A new Runnable with the fields configured.
Return type:

    RunnableSerializable

from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatOpenAI(max_tokens=20).configurable_fields(
    max_tokens=ConfigurableField(
        id="output_token_number",
        name="Max tokens in the output",
        description="The maximum number of tokens in the output",
    )
)

# max_tokens = 20
print(
    "max_tokens_20: ",
    model.invoke("tell me something about chess").content
)

# max_tokens = 200
print("max_tokens_200: ", model.with_config(
    configurable={"output_token_number": 200}
    ).invoke("tell me something about chess").content
)

get_num_tokens(text: str) â†’ int
[source]

    Get the number of tokens present in the text.

    Useful for checking if an input will fit in a modelâ€™s context window.

    Parameters:

        text (str) â€“ The string input to tokenize.
    Returns:

        The integer number of tokens in the text.
    Return type:

        int

get_num_tokens_from_messages(messages: list[BaseMessage]) â†’ int

    Get the number of tokens in the messages.

    Useful for checking if an input fits in a modelâ€™s context window.

    Parameters:

        messages (list[BaseMessage]) â€“ The message inputs to tokenize.
    Returns:

        The sum of the number of tokens across the messages.
    Return type:

        int

get_token_ids(text: str) â†’ list[int]

    Return the ordered ids of the tokens in a text.

    Parameters:

        text (str) â€“ The string input to tokenize.
    Returns:

        A list of ids corresponding to the tokens in the text, in order they occur

            in the text.

    Return type:

        list[int]

invoke(input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any) â†’ BaseMessage

    Transform a single input into an output. Override to implement.

    Parameters:

            input (LanguageModelInput) â€“ The input to the Runnable.

            config (Optional[RunnableConfig]) â€“ A config to use when invoking the Runnable. The config supports standard keys like â€˜tagsâ€™, â€˜metadataâ€™ for tracing purposes, â€˜max_concurrencyâ€™ for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details.

            stop (Optional[list[str]])

            kwargs (Any)

    Returns:

        The output of the Runnable.
    Return type:

        BaseMessage

stream(input: LanguageModelInput, config: RunnableConfig | None = None, *, stop: list[str] | None = None, **kwargs: Any) â†’ Iterator[BaseMessageChunk]

    Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output.

    Parameters:

            input (LanguageModelInput) â€“ The input to the Runnable.

            config (Optional[RunnableConfig]) â€“ The config to use for the Runnable. Defaults to None.

            kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.

            stop (Optional[list[str]])

    Yields:

        The output of the Runnable.
    Return type:

        Iterator[BaseMessageChunk]

with_alisteners(*, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None) â†’ Runnable[Input, Output]

Bind asynchronous lifecycle listeners to a Runnable, returning a new Runnable.

on_start: Asynchronously called before the Runnable starts running. on_end: Asynchronously called after the Runnable finishes running. on_error: Asynchronously called if the Runnable throws an error.

The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run.

Parameters:

        on_start (Optional[AsyncListener]) â€“ Asynchronously called before the Runnable starts running. Defaults to None.

        on_end (Optional[AsyncListener]) â€“ Asynchronously called after the Runnable finishes running. Defaults to None.

        on_error (Optional[AsyncListener]) â€“ Asynchronously called if the Runnable throws an error. Defaults to None.

Returns:

    A new Runnable with the listeners bound.
Return type:

    Runnable[Input, Output]

Example:

from langchain_core.runnables import RunnableLambda
import time

async def test_runnable(time_to_sleep : int):
    print(f"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}")
    await asyncio.sleep(time_to_sleep)
    print(f"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}")

async def fn_start(run_obj : Runnable):
    print(f"on start callback starts at {format_t(time.time())}
    await asyncio.sleep(3)
    print(f"on start callback ends at {format_t(time.time())}")

async def fn_end(run_obj : Runnable):
    print(f"on end callback starts at {format_t(time.time())}
    await asyncio.sleep(2)
    print(f"on end callback ends at {format_t(time.time())}")

runnable = RunnableLambda(test_runnable).with_alisteners(
    on_start=fn_start,
    on_end=fn_end
)
async def concurrent_runs():
    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))

asyncio.run(concurrent_runs())
Result:
on start callback starts at 2024-05-16T14:20:29.637053+00:00
on start callback starts at 2024-05-16T14:20:29.637150+00:00
on start callback ends at 2024-05-16T14:20:32.638305+00:00
on start callback ends at 2024-05-16T14:20:32.638383+00:00
Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00
Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00
Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00
on end callback starts at 2024-05-16T14:20:35.640534+00:00
Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00
on end callback starts at 2024-05-16T14:20:37.640574+00:00
on end callback ends at 2024-05-16T14:20:37.640654+00:00
on end callback ends at 2024-05-16T14:20:39.641751+00:00

with_config(config: RunnableConfig | None = None, **kwargs: Any) â†’ Runnable[Input, Output]

    Bind config to a Runnable, returning a new Runnable.

    Parameters:

            config (RunnableConfig | None) â€“ The config to bind to the Runnable.

            kwargs (Any) â€“ Additional keyword arguments to pass to the Runnable.

    Returns:

        A new Runnable with the config bound.
    Return type:

        Runnable[Input, Output]

with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class 'Exception'>,), exception_key: Optional[str] = None) â†’ RunnableWithFallbacksT[Input, Output]

Add fallbacks to a Runnable, returning a new Runnable.

The new Runnable will try the original Runnable, and then each fallback in order, upon failures.

Parameters:

        fallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.

        exceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle. Defaults to (Exception,).

        exception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key. If None, exceptions will not be passed to fallbacks. If used, the base Runnable and its fallbacks must accept a dictionary as input. Defaults to None.

Returns:

    A new Runnable that will try the original Runnable, and then each fallback in order, upon failures.
Return type:

    RunnableWithFallbacksT[Input, Output]

Example

from typing import Iterator

from langchain_core.runnables import RunnableGenerator


def _generate_immediate_error(input: Iterator) -> Iterator[str]:
    raise ValueError()
    yield ""


def _generate(input: Iterator) -> Iterator[str]:
    yield from "foo bar"


runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(
    [RunnableGenerator(_generate)]
    )
print(''.join(runnable.stream({}))) #foo bar

    Parameters:

            fallbacks (Sequence[Runnable[Input, Output]]) â€“ A sequence of runnables to try if the original Runnable fails.

            exceptions_to_handle (tuple[type[BaseException], ...]) â€“ A tuple of exception types to handle.

            exception_key (Optional[str]) â€“ If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key. If None, exceptions will not be passed to fallbacks. If used, the base Runnable and its fallbacks must accept a dictionary as input.

    Returns:

        A new Runnable that will try the original Runnable, and then each fallback in order, upon failures.
    Return type:

        RunnableWithFallbacksT[Input, Output]

with_listeners(*, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None) â†’ Runnable[Input, Output]

Bind lifecycle listeners to a Runnable, returning a new Runnable.

on_start: Called before the Runnable starts running, with the Run object. on_end: Called after the Runnable finishes running, with the Run object. on_error: Called if the Runnable throws an error, with the Run object.

The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run.

Parameters:

        on_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called before the Runnable starts running. Defaults to None.

        on_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called after the Runnable finishes running. Defaults to None.

        on_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) â€“ Called if the Runnable throws an error. Defaults to None.

Returns:

    A new Runnable with the listeners bound.
Return type:

    Runnable[Input, Output]

Example:

from langchain_core.runnables import RunnableLambda
from langchain_core.tracers.schemas import Run

import time

def test_runnable(time_to_sleep : int):
    time.sleep(time_to_sleep)

def fn_start(run_obj: Run):
    print("start_time:", run_obj.start_time)

def fn_end(run_obj: Run):
    print("end_time:", run_obj.end_time)

chain = RunnableLambda(test_runnable).with_listeners(
    on_start=fn_start,
    on_end=fn_end
)
chain.invoke(2)

with_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class 'Exception'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) â†’ Runnable[Input, Output]

Create a new Runnable that retries the original Runnable on exceptions.

Parameters:

        retry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on. Defaults to (Exception,).

        wait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time between retries. Defaults to True.

        stop_after_attempt (int) â€“ The maximum number of attempts to make before giving up. Defaults to 3.

Returns:

    A new Runnable that retries the original Runnable on exceptions.
Return type:

    Runnable[Input, Output]

Example:

from langchain_core.runnables import RunnableLambda

count = 0


def _lambda(x: int) -> None:
    global count
    count = count + 1
    if x == 1:
        raise ValueError("x is 1")
    else:
         pass


runnable = RunnableLambda(_lambda)
try:
    runnable.with_retry(
        stop_after_attempt=2,
        retry_if_exception_type=(ValueError,),
    ).invoke(1)
except ValueError:
    pass

assert (count == 2)

    Parameters:

            retry_if_exception_type (tuple[type[BaseException], ...]) â€“ A tuple of exception types to retry on

            wait_exponential_jitter (bool) â€“ Whether to add jitter to the wait time between retries

            stop_after_attempt (int) â€“ The maximum number of attempts to make before giving up

    Returns:

        A new Runnable that retries the original Runnable on exceptions.
    Return type:

        Runnable[Input, Output]

with_structured_output(schema: Dict | Type[BaseModel], *, include_raw: bool = False, **kwargs: Any) â†’ Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], Dict | BaseModel]
[source]

Model wrapper that returns outputs formatted to match the given schema.

Parameters:

        schema (Dict | Type[BaseModel]) â€“

        The output schema. Can be passed in as:

                an OpenAI function/tool schema,

                a JSON Schema,

                a TypedDict class (support added in 0.2.26),

                or a Pydantic class.

        If schema is a Pydantic class then the model output will be a Pydantic instance of that class, and the model-generated fields will be validated by the Pydantic class. Otherwise the model output will be a dict and will not be validated. See langchain_core.utils.function_calling.convert_to_openai_tool() for more on how to properly specify types and descriptions of schema fields when specifying a Pydantic or TypedDict class.

        Changed in version 0.2.26: Added support for TypedDict class.

        include_raw (bool) â€“ If False then only the parsed structured output is returned. If an error occurs during model output parsing it will be raised. If True then both the raw model response (a BaseMessage) and the parsed model response will be returned. If an error occurs during output parsing it will be caught and returned as well. The final output is always a dict with keys â€œrawâ€, â€œparsedâ€, and â€œparsing_errorâ€.

        kwargs (Any)

Returns:

    A Runnable that takes same inputs as a langchain_core.language_models.chat.BaseChatModel.

    If include_raw is False and schema is a Pydantic class, Runnable outputs an instance of schema (i.e., a Pydantic object).

    Otherwise, if include_raw is False then Runnable outputs a dict.

    If
    include_raw
    is True, then Runnable outputs a dict with keys:

            "raw": BaseMessage

            "parsed": None if there was a parsing error, otherwise the type depends on the schema as described above.

            "parsing_error": Optional[BaseException]

Return type:

    Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], Dict | BaseModel]

Example: Pydantic schema (include_raw=False):

    from pydantic import BaseModel

    class AnswerWithJustification(BaseModel):
        '''An answer to the user question along with justification for the answer.'''
        answer: str
        justification: str

    llm = ChatModel(model="model-name", temperature=0)
    structured_llm = llm.with_structured_output(AnswerWithJustification)

    structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")

    # -> AnswerWithJustification(
    #     answer='They weigh the same',
    #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'
    # )

Example: Pydantic schema (include_raw=True):

    from pydantic import BaseModel

    class AnswerWithJustification(BaseModel):
        '''An answer to the user question along with justification for the answer.'''
        answer: str
        justification: str

    llm = ChatModel(model="model-name", temperature=0)
    structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)

    structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")
    # -> {
    #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),
    #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),
    #     'parsing_error': None
    # }

Example: Dict schema (include_raw=False):

    from pydantic import BaseModel
    from langchain_core.utils.function_calling import convert_to_openai_tool

    class AnswerWithJustification(BaseModel):
        '''An answer to the user question along with justification for the answer.'''
        answer: str
        justification: str

    dict_schema = convert_to_openai_tool(AnswerWithJustification)
    llm = ChatModel(model="model-name", temperature=0)
    structured_llm = llm.with_structured_output(dict_schema)

    structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")
    # -> {
    #     'answer': 'They weigh the same',
    #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'
    # }

with_types(*, input_type: type[Input] | None = None, output_type: type[Output] | None = None) â†’ Runnable[Input, Output]

    Bind input and output types to a Runnable, returning a new Runnable.

    Parameters:

            input_type (type[Input] | None) â€“ The input type to bind to the Runnable. Defaults to None.

            output_type (type[Output] | None) â€“ The output type to bind to the Runnable. Defaults to None.

    Returns:

        A new Runnable with the types bound.
    Return type:

        Runnable[Input, Output]




Document loaders

DocumentLoaders load data into the standard LangChain Document format.

Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method. An example use case is as follows:

from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
data = loader.load()

ChatGoogleGenerativeAIError
class langchain_google_genai.chat_models.ChatGoogleGenerativeAIError
[source]

    Custom exception class for errors associated with the Google GenAI API.

    This exception is raised when there are specific issues related to the Google genai API usage in the ChatGoogleGenerativeAI class, such as unsupported message types or roles.
GoogleGenerativeAIEmbeddings
class langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings
[source]

Bases: BaseModel, Embeddings

Google Generative AI Embeddings.

To use, you must have either:

        The GOOGLE_API_KEY` environment variable set with your API key, or

        Pass your API key using the google_api_key kwarg to the ChatGoogle constructor.

Example

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
embeddings.embed_query("What's our Q1 revenue?")

Create a new model by parsing and validating input data from keyword arguments.

Raises [ValidationError][pydantic_core.ValidationError] if the input data cannot be validated to form a valid model.

self is explicitly positional-only to allow self as a field name.

param client_options: Dict | None = None

    A dictionary of client options to pass to the Google API client, such as api_endpoint.

param credentials: Any = None

    The default custom credentials (google.auth.credentials.Credentials) to use when making API calls. If not provided, credentials will be ascertained from the GOOGLE_API_KEY envvar

param google_api_key: SecretStr | None [Optional]

    The Google API key to use. If not provided, the GOOGLE_API_KEY environment variable will be used.

param model: str [Required]

    The name of the embedding model to use. Example: models/embedding-001

param request_options: Dict | None = None

    A dictionary of request options to pass to the Google API client.Example: {â€˜timeoutâ€™: 10}

param task_type: str | None = None

    The task type. Valid options include: task_type_unspecified, retrieval_query, retrieval_document, semantic_similarity, classification, and clustering

param transport: str | None = None

    A string, one of: [rest, grpc, grpc_asyncio].

async aembed_documents(texts: list[str]) â†’ list[list[float]]

    Asynchronous Embed search docs.

    Parameters:

        texts (list[str]) â€“ List of text to embed.
    Returns:

        List of embeddings.
    Return type:

        list[list[float]]

async aembed_query(text: str) â†’ list[float]

    Asynchronous Embed query text.

    Parameters:

        text (str) â€“ Text to embed.
    Returns:

        Embedding.
    Return type:

        list[float]

embed_documents(texts: List[str], *, batch_size: int = 100, task_type: str | None = None, titles: List[str] | None = None, output_dimensionality: int | None = None) â†’ List[List[float]]
[source]

    Embed a list of strings. Google Generative AI currently sets a max batch size of 100 strings.

    Parameters:

            texts (List[str]) â€“ List[str] The list of strings to embed.

            batch_size (int) â€“ [int] The batch size of embeddings to send to the model

            task_type (str | None) â€“ task_type (https://ai.google.dev/api/rest/v1/TaskType)

            titles (List[str] | None) â€“ An optional list of titles for texts provided.

            RETRIEVAL_DOCUMENT. (Only applicable when TaskType is)

            output_dimensionality (int | None) â€“ Optional reduced dimension for the output embedding.

            https â€“ //ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest

    Returns:

        List of embeddings, one for each text.
    Return type:

        List[List[float]]

embed_query(text: str, task_type: str | None = None, title: str | None = None, output_dimensionality: int | None = None) â†’ List[float]
[source]

    Embed a text.

    Parameters:

            text (str) â€“ The text to embed.

            task_type (str | None) â€“ task_type (https://ai.google.dev/api/rest/v1/TaskType)

            title (str | None) â€“ An optional title for the text.

            RETRIEVAL_DOCUMENT. (Only applicable when TaskType is)

            output_dimensionality (int | None) â€“ Optional reduced dimension for the output embedding.

            https â€“ //ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest

    Returns:

        Embedding for the text.
    Return type:

        List[float]

ServerSideEmbedding
class langchain_google_genai.google_vector_store.ServerSideEmbedding
[source]

Do nothing embedding model where the embedding is done by the server.

Methods

__init__()
	

aembed_documents(texts)
	

Asynchronous Embed search docs.

aembed_query(text)
	

Asynchronous Embed query text.

embed_documents(texts)
	

Embed search docs.

embed_query(text)
	

Embed query text.

__init__()

async aembed_documents(texts: list[str]) â†’ list[list[float]]

    Asynchronous Embed search docs.

    Parameters:

        texts (list[str]) â€“ List of text to embed.
    Returns:

        List of embeddings.
    Return type:

        list[list[float]]

async aembed_query(text: str) â†’ list[float]

    Asynchronous Embed query text.

    Parameters:

        text (str) â€“ Text to embed.
    Returns:

        Embedding.
    Return type:

        list[float]

embed_documents(texts: List[str]) â†’ List[List[float]]
[source]

    Embed search docs.

    Parameters:

        texts (List[str]) â€“ List of text to embed.
    Returns:

        List of embeddings.
    Return type:

        List[List[float]]

embed_query(text: str) â†’ List[float]
[source]

    Embed query text.

    Parameters:

        text (str) â€“ Text to embed.
    Returns:

        Embedding.
    Return type:

        List[float]


GoogleVectorStore
class langchain_google_genai.google_vector_store.GoogleVectorStore(*, corpus_id: str, document_id: str | None = None, **kwargs: Any)
[source]

Google GenerativeAI Vector Store.

Currently, it computes the embedding vectors on the server side.

Example: Add texts to an existing corpus.

    store = GoogleVectorStore(corpus_id=â€123â€) store.add_documents(documents, document_id=â€456â€)

Example: Create a new corpus.

    store = GoogleVectorStore.create_corpus(

        corpus_id=â€123â€, display_name=â€My Google corpusâ€)

Example: Query the corpus for relevant passages.

    store.as_retriever() .get_relevant_documents(â€œWho caught the gingerbread man?â€)

Example: Ask the corpus for grounded responses!

    aqa = store.as_aqa() response = aqa.invoke(â€œWho caught the gingerbread man?â€) print(response.answer) print(response.attributed_passages) print(response.answerability_probability)

You can also operate at Googleâ€™s Document level.

Example: Add texts to an existing Google Vector Store Document.

    doc_store = GoogleVectorStore(corpus_id=â€123â€, document_id=â€456â€) doc_store.add_documents(documents)

Example: Create a new Google Vector Store Document.

    doc_store = GoogleVectorStore.create_document(

        corpus_id=â€123â€, document_id=â€456â€, display_name=â€My Google documentâ€)

Example: Query the Google document.

    doc_store.as_retriever() .get_relevant_documents(â€œWho caught the gingerbread man?â€)

For more details, see the classâ€™s methods.

Returns an existing Google Semantic Retriever corpus or document.

If just the corpus ID is provided, the vector store operates over all documents within that corpus.

If the document ID is provided, the vector store operates over just that document.

Raises:

    DoesNotExistsException if the IDs do not match to anything on Google â€“ server. In this case, consider using create_corpus or create_document to create one.
Parameters:

        corpus_id (str)

        document_id (str | None)

        kwargs (Any)

Attributes

corpus_id
	

Returns the corpus ID managed by this vector store.

document_id
	

Returns the document ID managed by this vector store.

embeddings
	

Access the query embedding object if available.

name
	

Returns the name of the Google entity.

Methods

__init__(*, corpus_id[, document_id])
	

Returns an existing Google Semantic Retriever corpus or document.

aadd_documents(documents, **kwargs)
	

Async run more documents through the embeddings and add to the vectorstore.

aadd_texts(texts[, metadatas, ids])
	

Async run more texts through the embeddings and add to the vectorstore.

add_documents(documents, **kwargs)
	

Add or update documents in the vectorstore.

add_texts(texts[, metadatas, document_id])
	

Add texts to the vector store.

adelete([ids])
	

Async delete by vector ID or other criteria.

afrom_documents(documents, embedding, **kwargs)
	

Async return VectorStore initialized from documents and embeddings.

afrom_texts(texts, embedding[, metadatas, ids])
	

Async return VectorStore initialized from texts and embeddings.

aget_by_ids(ids, /)
	

Async get documents by their IDs.

amax_marginal_relevance_search(query[, k, ...])
	

Async return docs selected using the maximal marginal relevance.

amax_marginal_relevance_search_by_vector(...)
	

Async return docs selected using the maximal marginal relevance.

as_aqa(**kwargs)
	

Construct a Google Generative AI AQA engine.

as_retriever(**kwargs)
	

Return VectorStoreRetriever initialized from this VectorStore.

asearch(query, search_type, **kwargs)
	

Async return docs most similar to query using a specified search type.

asimilarity_search(query[, k])
	

Async return docs most similar to query.

asimilarity_search_by_vector(embedding[, k])
	

Async return docs most similar to embedding vector.

asimilarity_search_with_relevance_scores(query)
	

Async return docs and relevance scores in the range [0, 1].

asimilarity_search_with_score(*args, **kwargs)
	

Async run similarity search with distance.

create_corpus([corpus_id, display_name])
	

Create a Google Semantic Retriever corpus.

create_document(corpus_id[, document_id, ...])
	

Create a Google Semantic Retriever document.

delete([ids])
	

Delete chunnks.

from_documents(documents, embedding, **kwargs)
	

Return VectorStore initialized from documents and embeddings.

from_texts(texts[, embedding, metadatas, ...])
	

Returns a vector store of an existing document with the specified text.

get_by_ids(ids, /)
	

Get documents by their IDs.

max_marginal_relevance_search(query[, k, ...])
	

Return docs selected using the maximal marginal relevance.

max_marginal_relevance_search_by_vector(...)
	

Return docs selected using the maximal marginal relevance.

search(query, search_type, **kwargs)
	

Return docs most similar to query using a specified search type.

similarity_search(query[, k, filter])
	

Search the vector store for relevant texts.

similarity_search_by_vector(embedding[, k])
	

Return docs most similar to embedding vector.

similarity_search_with_relevance_scores(query)
	

Return docs and relevance scores in the range [0, 1].

similarity_search_with_score(query[, k, filter])
	

Run similarity search with distance.

__init__(*, corpus_id: str, document_id: str | None = None, **kwargs: Any)
[source]

    Returns an existing Google Semantic Retriever corpus or document.

    If just the corpus ID is provided, the vector store operates over all documents within that corpus.

    If the document ID is provided, the vector store operates over just that document.

    Raises:

        DoesNotExistsException if the IDs do not match to anything on Google â€“ server. In this case, consider using create_corpus or create_document to create one.
    Parameters:

            corpus_id (str)

            document_id (str | None)

            kwargs (Any)

async aadd_documents(documents: list[Document], **kwargs: Any) â†’ list[str]

    Async run more documents through the embeddings and add to the vectorstore.

    Parameters:

            documents (list[Document]) â€“ Documents to add to the vectorstore.

            kwargs (Any) â€“ Additional keyword arguments.

    Returns:

        List of IDs of the added texts.
    Raises:

        ValueError â€“ If the number of IDs does not match the number of documents.
    Return type:

        list[str]

async aadd_texts(texts: Iterable[str], metadatas: list[dict] | None = None, *, ids: list[str] | None = None, **kwargs: Any) â†’ list[str]

    Async run more texts through the embeddings and add to the vectorstore.

    Parameters:

            texts (Iterable[str]) â€“ Iterable of strings to add to the vectorstore.

            metadatas (list[dict] | None) â€“ Optional list of metadatas associated with the texts. Default is None.

            ids (list[str] | None) â€“ Optional list

            **kwargs (Any) â€“ vectorstore specific parameters.

    Returns:

        List of ids from adding the texts into the vectorstore.
    Raises:

            ValueError â€“ If the number of metadatas does not match the number of texts.

            ValueError â€“ If the number of ids does not match the number of texts.

    Return type:

        list[str]

add_documents(documents: list[Document], **kwargs: Any) â†’ list[str]

    Add or update documents in the vectorstore.

    Parameters:

            documents (list[Document]) â€“ Documents to add to the vectorstore.

            kwargs (Any) â€“ Additional keyword arguments. if kwargs contains ids and documents contain ids, the ids in the kwargs will receive precedence.

    Returns:

        List of IDs of the added texts.
    Raises:

        ValueError â€“ If the number of ids does not match the number of documents.
    Return type:

        list[str]

add_texts(texts: Iterable[str], metadatas: List[Dict[str, Any]] | None = None, *, document_id: str | None = None, **kwargs: Any) â†’ List[str]
[source]

    Add texts to the vector store.

    If the vector store points to a corpus (instead of a document), you must also provide a document_id.

    Returns:

        Chunkâ€™s names created on Google servers.
    Parameters:

            texts (Iterable[str])

            metadatas (List[Dict[str, Any]] | None)

            document_id (str | None)

            kwargs (Any)

    Return type:

        List[str]

async adelete(ids: List[str] | None = None, **kwargs: Any) â†’ bool | None
[source]

    Async delete by vector ID or other criteria.

    Parameters:

            ids (List[str] | None) â€“ List of ids to delete. If None, delete all. Default is None.

            **kwargs (Any) â€“ Other keyword arguments that subclasses might use.

    Returns:

        True if deletion is successful, False otherwise, None if not implemented.
    Return type:

        Optional[bool]

async classmethod afrom_documents(documents: list[Document], embedding: Embeddings, **kwargs: Any) â†’ VST

    Async return VectorStore initialized from documents and embeddings.

    Parameters:

            documents (list[Document]) â€“ List of Documents to add to the vectorstore.

            embedding (Embeddings) â€“ Embedding function to use.

            kwargs (Any) â€“ Additional keyword arguments.

    Returns:

        VectorStore initialized from documents and embeddings.
    Return type:

        VectorStore

async classmethod afrom_texts(texts: list[str], embedding: Embeddings, metadatas: list[dict] | None = None, *, ids: list[str] | None = None, **kwargs: Any) â†’ VST

    Async return VectorStore initialized from texts and embeddings.

    Parameters:

            texts (list[str]) â€“ Texts to add to the vectorstore.

            embedding (Embeddings) â€“ Embedding function to use.

            metadatas (list[dict] | None) â€“ Optional list of metadatas associated with the texts. Default is None.

            ids (list[str] | None) â€“ Optional list of IDs associated with the texts.

            kwargs (Any) â€“ Additional keyword arguments.

    Returns:

        VectorStore initialized from texts and embeddings.
    Return type:

        VectorStore

async aget_by_ids(ids: Sequence[str], /) â†’ list[Document]

    Async get documents by their IDs.

    The returned documents are expected to have the ID field set to the ID of the document in the vector store.

    Fewer documents may be returned than requested if some IDs are not found or if there are duplicated IDs.

    Users should not assume that the order of the returned documents matches the order of the input IDs. Instead, users should rely on the ID field of the returned documents.

    This method should NOT raise exceptions if no documents are found for some IDs.

    Parameters:

        ids (Sequence[str]) â€“ List of ids to retrieve.
    Returns:

        List of Documents.
    Return type:

        list[Document]

    Added in version 0.2.11.

async amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) â†’ list[Document]

    Async return docs selected using the maximal marginal relevance.

    Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

    Parameters:

            query (str) â€“ Text to look up documents similar to.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            fetch_k (int) â€“ Number of Documents to fetch to pass to MMR algorithm. Default is 20.

            lambda_mult (float) â€“ Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.

            kwargs (Any)

    Returns:

        List of Documents selected by maximal marginal relevance.
    Return type:

        list[Document]

async amax_marginal_relevance_search_by_vector(embedding: list[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) â†’ list[Document]

    Async return docs selected using the maximal marginal relevance.

    Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

    Parameters:

            embedding (list[float]) â€“ Embedding to look up documents similar to.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            fetch_k (int) â€“ Number of Documents to fetch to pass to MMR algorithm. Default is 20.

            lambda_mult (float) â€“ Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents selected by maximal marginal relevance.
    Return type:

        list[Document]

as_aqa(**kwargs: Any) â†’ Runnable[str, AqaOutput]
[source]

    Construct a Google Generative AI AQA engine.

    All arguments are optional.

    Parameters:

            answer_style â€“ See google.ai.generativelanguage.GenerateAnswerRequest.AnswerStyle.

            safety_settings â€“ See google.ai.generativelanguage.SafetySetting.

            temperature â€“ 0.0 to 1.0.

            kwargs (Any)

    Return type:

        Runnable[str, AqaOutput]

as_retriever(**kwargs: Any) â†’ VectorStoreRetriever

Return VectorStoreRetriever initialized from this VectorStore.

Parameters:

    **kwargs (Any) â€“

    Keyword arguments to pass to the search function. Can include: search_type (Optional[str]): Defines the type of search that

        the Retriever should perform. Can be â€œsimilarityâ€ (default), â€œmmrâ€, or â€œsimilarity_score_thresholdâ€.

    search_kwargs (Optional[Dict]): Keyword arguments to pass to the

        search function. Can include things like:

            k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold

                for similarity_score_threshold

            fetch_k: Amount of documents to pass to MMR algorithm

                (Default: 20)
            lambda_mult: Diversity of results returned by MMR;

                1 for minimum diversity and 0 for maximum. (Default: 0.5)

            filter: Filter by document metadata

Returns:

    Retriever class for VectorStore.
Return type:

    VectorStoreRetriever

Examples:

# Retrieve more documents with higher diversity
# Useful if your dataset has many similar documents
docsearch.as_retriever(
    search_type="mmr",
    search_kwargs={'k': 6, 'lambda_mult': 0.25}
)

# Fetch more documents for the MMR algorithm to consider
# But only return the top 5
docsearch.as_retriever(
    search_type="mmr",
    search_kwargs={'k': 5, 'fetch_k': 50}
)

# Only retrieve documents that have a relevance score
# Above a certain threshold
docsearch.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={'score_threshold': 0.8}
)

# Only get the single most similar document from the dataset
docsearch.as_retriever(search_kwargs={'k': 1})

# Use a filter to only retrieve documents from a specific paper
docsearch.as_retriever(
    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}
)

async asearch(query: str, search_type: str, **kwargs: Any) â†’ list[Document]

    Async return docs most similar to query using a specified search type.

    Parameters:

            query (str) â€“ Input text.

            search_type (str) â€“ Type of search to perform. Can be â€œsimilarityâ€, â€œmmrâ€, or â€œsimilarity_score_thresholdâ€.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents most similar to the query.
    Raises:

        ValueError â€“ If search_type is not one of â€œsimilarityâ€, â€œmmrâ€, or â€œsimilarity_score_thresholdâ€.
    Return type:

        list[Document]

async asimilarity_search(query: str, k: int = 4, **kwargs: Any) â†’ list[Document]

    Async return docs most similar to query.

    Parameters:

            query (str) â€“ Input text.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents most similar to the query.
    Return type:

        list[Document]

async asimilarity_search_by_vector(embedding: list[float], k: int = 4, **kwargs: Any) â†’ list[Document]

    Async return docs most similar to embedding vector.

    Parameters:

            embedding (list[float]) â€“ Embedding to look up documents similar to.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents most similar to the query vector.
    Return type:

        list[Document]

async asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any) â†’ list[tuple[Document, float]]

    Async return docs and relevance scores in the range [0, 1].

    0 is dissimilar, 1 is most similar.

    Parameters:

            query (str) â€“ Input text.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            **kwargs (Any) â€“

            kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to

                filter the resulting set of retrieved docs

    Returns:

        List of Tuples of (doc, similarity_score)
    Return type:

        list[tuple[Document, float]]

async asimilarity_search_with_score(*args: Any, **kwargs: Any) â†’ list[tuple[Document, float]]

    Async run similarity search with distance.

    Parameters:

            *args (Any) â€“ Arguments to pass to the search method.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Tuples of (doc, similarity_score).
    Return type:

        list[tuple[Document, float]]

classmethod create_corpus(corpus_id: str | None = None, display_name: str | None = None) â†’ GoogleVectorStore
[source]

    Create a Google Semantic Retriever corpus.

    Parameters:

            corpus_id (str | None) â€“ The ID to use to create the new corpus. If not provided, Google server will provide one.

            display_name (str | None) â€“ The title of the new corpus. If not provided, Google server will provide one.

    Returns:

        An instance of vector store that points to the newly created corpus.
    Return type:

        GoogleVectorStore

classmethod create_document(corpus_id: str, document_id: str | None = None, display_name: str | None = None, metadata: Dict[str, Any] | None = None) â†’ GoogleVectorStore
[source]

    Create a Google Semantic Retriever document.

    Parameters:

            corpus_id (str) â€“ ID of an existing corpus.

            document_id (str | None) â€“ The ID to use to create the new Google Semantic Retriever document. If not provided, Google server will provide one.

            display_name (str | None) â€“ The title of the new document. If not provided, Google server will provide one.

            metadata (Dict[str, Any] | None)

    Returns:

        An instance of vector store that points to the newly created document.
    Return type:

        GoogleVectorStore

delete(ids: List[str] | None = None, **kwargs: Any) â†’ bool | None
[source]

    Delete chunnks.

    Note that the â€œidsâ€ are not corpus ID or document ID. Rather, these are the entity names returned by add_texts.

    Returns:

        True if successful. Otherwise, you should get an exception anyway.
    Parameters:

            ids (List[str] | None)

            kwargs (Any)

    Return type:

        bool | None

classmethod from_documents(documents: list[Document], embedding: Embeddings, **kwargs: Any) â†’ VST

    Return VectorStore initialized from documents and embeddings.

    Parameters:

            documents (list[Document]) â€“ List of Documents to add to the vectorstore.

            embedding (Embeddings) â€“ Embedding function to use.

            kwargs (Any) â€“ Additional keyword arguments.

    Returns:

        VectorStore initialized from documents and embeddings.
    Return type:

        VectorStore

classmethod from_texts(texts: List[str], embedding: Embeddings | None = None, metadatas: List[dict[str, Any]] | None = None, *, corpus_id: str | None = None, document_id: str | None = None, **kwargs: Any) â†’ GoogleVectorStore
[source]

    Returns a vector store of an existing document with the specified text.

    Parameters:

            corpus_id (str | None) â€“ REQUIRED. Must be an existing corpus.

            document_id (str | None) â€“ REQUIRED. Must be an existing document.

            texts (List[str]) â€“ Texts to be loaded into the vector store.

            embedding (Embeddings | None)

            metadatas (List[dict[str, Any]] | None)

            kwargs (Any)

    Returns:

        A vector store pointing to the specified Google Semantic Retriever Document.
    Raises:

        DoesNotExistsException if the IDs do not match to anything at â€“ Google server.
    Return type:

        GoogleVectorStore

get_by_ids(ids: Sequence[str], /) â†’ list[Document]

    Get documents by their IDs.

    The returned documents are expected to have the ID field set to the ID of the document in the vector store.

    Fewer documents may be returned than requested if some IDs are not found or if there are duplicated IDs.

    Users should not assume that the order of the returned documents matches the order of the input IDs. Instead, users should rely on the ID field of the returned documents.

    This method should NOT raise exceptions if no documents are found for some IDs.

    Parameters:

        ids (Sequence[str]) â€“ List of ids to retrieve.
    Returns:

        List of Documents.
    Return type:

        list[Document]

    Added in version 0.2.11.

max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) â†’ list[Document]

    Return docs selected using the maximal marginal relevance.

    Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

    Parameters:

            query (str) â€“ Text to look up documents similar to.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            fetch_k (int) â€“ Number of Documents to fetch to pass to MMR algorithm. Default is 20.

            lambda_mult (float) â€“ Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents selected by maximal marginal relevance.
    Return type:

        list[Document]

max_marginal_relevance_search_by_vector(embedding: list[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) â†’ list[Document]

    Return docs selected using the maximal marginal relevance.

    Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

    Parameters:

            embedding (list[float]) â€“ Embedding to look up documents similar to.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            fetch_k (int) â€“ Number of Documents to fetch to pass to MMR algorithm. Default is 20.

            lambda_mult (float) â€“ Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents selected by maximal marginal relevance.
    Return type:

        list[Document]

search(query: str, search_type: str, **kwargs: Any) â†’ list[Document]

    Return docs most similar to query using a specified search type.

    Parameters:

            query (str) â€“ Input text

            search_type (str) â€“ Type of search to perform. Can be â€œsimilarityâ€, â€œmmrâ€, or â€œsimilarity_score_thresholdâ€.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents most similar to the query.
    Raises:

        ValueError â€“ If search_type is not one of â€œsimilarityâ€, â€œmmrâ€, or â€œsimilarity_score_thresholdâ€.
    Return type:

        list[Document]

similarity_search(query: str, k: int = 4, filter: Dict[str, Any] | None = None, **kwargs: Any) â†’ List[Document]
[source]

    Search the vector store for relevant texts.

    Parameters:

            query (str)

            k (int)

            filter (Dict[str, Any] | None)

            kwargs (Any)

    Return type:

        List[Document]

similarity_search_by_vector(embedding: list[float], k: int = 4, **kwargs: Any) â†’ list[Document]

    Return docs most similar to embedding vector.

    Parameters:

            embedding (list[float]) â€“ Embedding to look up documents similar to.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            **kwargs (Any) â€“ Arguments to pass to the search method.

    Returns:

        List of Documents most similar to the query vector.
    Return type:

        list[Document]

similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any) â†’ list[tuple[Document, float]]

    Return docs and relevance scores in the range [0, 1].

    0 is dissimilar, 1 is most similar.

    Parameters:

            query (str) â€“ Input text.

            k (int) â€“ Number of Documents to return. Defaults to 4.

            **kwargs (Any) â€“

            kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to

                filter the resulting set of retrieved docs.

    Returns:

        List of Tuples of (doc, similarity_score).
    Return type:

        list[tuple[Document, float]]

similarity_search_with_score(query: str, k: int = 4, filter: Dict[str, Any] | None = None, **kwargs: Any) â†’ List[Tuple[Document, float]]
[source]

    Run similarity search with distance.

    Parameters:

            query (str)

            k (int)

            filter (Dict[str, Any] | None)

            kwargs (Any)

    Return type:

        List[Tuple[Document, float]]

Conceptual guide

This section contains introductions to key parts of LangChain.
Architecture

LangChain as a framework consists of a number of packages.
langchain-core

This package contains base abstractions of different components and ways to compose them together. The interfaces for core components like LLMs, vector stores, retrievers and more are defined here. No third party integrations are defined here. The dependencies are kept purposefully very lightweight.
langchain

The main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive architecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.
langchain-community

This package contains third party integrations that are maintained by the LangChain community. Key partner packages are separated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All dependencies in this package are optional to keep the package as lightweight as possible.
Partner packages

While the long tail of integrations is in langchain-community, we split popular integrations into their own packages (e.g. langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.
langgraph

langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.

LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.
langserve

A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.
LangSmith

A developer platform that lets you debug, test, evaluate, and monitor LLM applications.
Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.
LangChain Expression Language (LCEL)

LangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€ chain to the most complex chains (weâ€™ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:

    First-class streaming support: When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.

    Async support: Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.

    Optimized parallel execution: Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.

    Retries and fallbacks: Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. Weâ€™re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.

    Access intermediate results: For more complex chains itâ€™s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and itâ€™s available on every LangServe server.

    Input and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.

    Seamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.

LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and ConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety of viable models emerge, customization has become more and more important.

If you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.

For guides on how to do specific tasks with LCEL, check out the relevant how-to guides.
Runnable interface

To make it as easy as possible to create custom chains, we've implemented a "Runnable" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about below.

This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:

    stream: stream back chunks of the response
    invoke: call the chain on an input
    batch: call the chain on a list of inputs

These also have corresponding async methods that should be used with asyncio await syntax for concurrency:

    astream: stream back chunks of the response async
    ainvoke: call the chain on an input async
    abatch: call the chain on a list of inputs async
    astream_log: stream back intermediate steps as they happen, in addition to the final response
    astream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)

The input type and output type varies by component:
Component	Input Type	Output Type
Prompt	Dictionary	PromptValue
ChatModel	Single string, list of chat messages or a PromptValue	ChatMessage
LLM	Single string, list of chat messages or a PromptValue	String
OutputParser	The output of an LLM or ChatModel	Depends on the parser
Retriever	Single string	List of Documents
Tool	Single string or dictionary, depending on the tool	Depends on the tool

All runnables expose input and output schemas to inspect the inputs and outputs:

    input_schema: an input Pydantic model auto-generated from the structure of the Runnable
    output_schema: an output Pydantic model auto-generated from the structure of the Runnable

Components

LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.
Chat models

Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.

Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input. This means you can easily use chat models in place of LLMs.

When a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.

LangChain does not host any Chat Models, rather we rely on third party integrations.

We have some standardized parameters when constructing ChatModels:

    model: the name of the model
    temperature: the sampling temperature
    timeout: request timeout
    max_tokens: max tokens to generate
    stop: default stop sequences
    max_retries: max number of times to retry requests
    api_key: API key for the model provider
    base_url: endpoint to send requests to

Some important things to note:

    standard params only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.
    standard params are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.

ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model.
important

Some chat models have been fine-tuned for tool calling and provide a dedicated API for it. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the tool calling section for more information.

For specifics on how to use chat models, see the relevant how-to guides here.
Multimodality

Some chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning model providers haven't standardized on the "best" way to define the API. Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.

In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.

For specifics on how to use multimodal models, see the relevant how-to guides here.

For a full list of LangChain model providers with multimodal models, check out this table.
LLMs
caution

Pure text-in/text-out LLMs tend to be older or lower-level. Many new popular models are best used as chat completion models, even for non-chat use cases.

You are probably looking for the section above instead.

Language models that takes a string as input and returns a string. These are traditionally older models (newer models generally are Chat Models, see above).

Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input. This gives them the same interface as Chat Models. When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.

LangChain does not host any LLMs, rather we rely on third party integrations.

For specifics on how to use LLMs, see the how-to guides.
Messages

Some language models take a list of messages as input and return a message. There are a few different types of messages. All messages have a role, content, and response_metadata property.

The role describes WHO is saying the message. The standard roles are "user", "assistant", "system", and "tool". LangChain has different message classes for different roles.

The content property describes the content of the message. This can be a few different things:

    A string (most models deal with this type of content)
    A List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and that input location)

Optionally, messages can have a name property which allows for differentiating between multiple speakers with the same role. For example, if there are two users in the chat history it can be useful to differentiate between them. Not all models support this.
HumanMessage

This represents a message with role "user".
AIMessage

This represents a message with role "assistant". In addition to the content property, these messages also have:

response_metadata

The response_metadata property contains additional metadata about the response. The data here is often specific to each model provider. This is where information like log-probs and token usage may be stored.

tool_calls

These represent a decision from a language model to call a tool. They are included as part of an AIMessage output. They can be accessed from there with the .tool_calls property.

This property returns a list of ToolCalls. A ToolCall is a dictionary with the following arguments:

    name: The name of the tool that should be called.
    args: The arguments to that tool.
    id: The id of that tool call.

SystemMessage

This represents a message with role "system", which tells the model how to behave. Not every model provider supports this.
ToolMessage

This represents a message with role "tool", which contains the result of calling a tool. In addition to role and content, this message has:

    a tool_call_id field which conveys the id of the call to the tool that was called to produce this result.
    an artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.

With most chat models, a ToolMessage can only appear in the chat history after an AIMessage that has a populated tool_calls field.
(Legacy) FunctionMessage

This is a legacy message type, corresponding to OpenAI's legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API.

This represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.
Prompt templates

Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.

Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.

Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates:
String PromptTemplates

These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:

from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")

prompt_template.invoke({"topic": "cats"})

API Reference:PromptTemplate
ChatPromptTemplates

These prompt templates are used to format a list of messages. These "templates" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:

from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

prompt_template.invoke({"topic": "cats"})

API Reference:ChatPromptTemplate

In the above example, this ChatPromptTemplate will construct two messages when called. The first is a system message, that has no variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in.
MessagesPlaceholder

This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder.

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})

API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage

This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in). This is useful for letting a list of messages be slotted into a particular spot.

An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("placeholder", "{msgs}") # <-- This is the changed part
])

For specifics on how to use prompt templates, see the relevant how-to guides here.
Example selectors

One common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as few-shot prompting. This gives the language model concrete examples of how it should behave. Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them. Example Selectors are classes responsible for selecting and then formatting examples into prompts.

For specifics on how to use example selectors, see the relevant how-to guides here.
Output parsers
note

The information here refers to parsers that take a text output from a model try to parse it into a more structured representation. More and more models are supporting function (or tool) calling, which handles this automatically. It is recommended to use function/tool calling rather than output parsing. See documentation for that here.

Output parser is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.

LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:

    Name: The name of the output parser
    Supports Streaming: Whether the output parser supports streaming.
    Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.
    Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.
    Input Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.
    Output Type: The output type of the object returned by the parser.
    Description: Our commentary on this output parser and when to use it.

Name	Supports Streaming	Has Format Instructions	Calls LLM	Input Type	Output Type	Description
JSON	âœ…	âœ…		str | Message	JSON object	Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.
XML	âœ…	âœ…		str | Message	dict	Returns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).
CSV	âœ…	âœ…		str | Message	List[str]	Returns a list of comma separated values.
OutputFixing			âœ…	str | Message		Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.
RetryWithError			âœ…	str | Message		Wraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.
Pydantic		âœ…		str | Message	pydantic.BaseModel	Takes a user defined Pydantic model and returns data in that format.
YAML		âœ…		str | Message	pydantic.BaseModel	Takes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.
PandasDataFrame		âœ…		str | Message	dict	Useful for doing operations with pandas DataFrames.
Enum		âœ…		str | Message	Enum	Parses response into one of the provided enum values.
Datetime		âœ…		str | Message	datetime.datetime	Parses response into a datetime string.
Structured		âœ…		str | Message	Dict[str, str]	An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.

For specifics on how to use output parsers, see the relevant how-to guides here.
Chat history

Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly.

The concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain. This ChatHistory will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future interactions will then load those messages and pass them into the chain as part of the input.
Documents

A Document object in LangChain contains information about some data. It has two attributes:

    page_content: str: The content of this document. Currently is only a string.
    metadata: dict: Arbitrary metadata associated with this document. Can track the document id, file name, etc.

Document loaders

These classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.

Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method. An example use case is as follows:

from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
data = loader.load()

API Reference:CSVLoader

For specifics on how to use document loaders, see the relevant how-to guides here.
Text splitters

Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.

When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What "semantically related" means could depend on the type of text. This notebook showcases several ways to do that.

At a high level, text splitters work as following:

    Split the text up into small, semantically meaningful chunks (often sentences).
    Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).
    Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).

That means there are two different axes along which you can customize your text splitter:

    How the text is split
    How the chunk size is measured

For specifics on how to use text splitters, see the relevant how-to guides here.
Embedding models

Embedding models create a vector representation of a piece of text. You can think of a vector as an array of numbers that captures the semantic meaning of the text. By representing the text in this way, you can perform mathematical operations that allow you to do things like search for other pieces of text that are most similar in meaning. These natural language search capabilities underpin many types of context retrieval, where we provide an LLM with the relevant data it needs to effectively respond to a query.

The Embeddings class is a class designed for interfacing with text embedding models. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.

The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).

For specifics on how to use embedding models, see the relevant how-to guides here.
Vector stores

One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.

Most vector stores can also store metadata about embedded vectors and support filtering on that metadata before similarity search, allowing you more control over returned documents.

Vector stores can be converted to the retriever interface by doing:

vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()

For specifics on how to use vector stores, see the relevant how-to guides here.
Retrievers

A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.

Retrievers accept a string query as input and return a list of Document's as output.

For specifics on how to use retrievers, see the relevant how-to guides here.
Key-value stores

For some techniques, such as indexing and retrieval with multiple vectors per document or caching embeddings, having a form of key-value (KV) storage is helpful.

LangChain includes a BaseStore interface, which allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a more specific BaseStore[str, bytes] instance that stores binary data (referred to as a ByteStore), and internally take care of encoding and decoding data for their specific needs.

This means that as a user, you only need to think about one type of store rather than different ones for different types of data.
Interface

All BaseStores support the following interface. Note that the interface allows for modifying multiple key-value pairs at once:

    mget(key: Sequence[str]) -> List[Optional[bytes]]: get the contents of multiple keys, returning None if the key does not exist
    mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None: set the contents of multiple keys
    mdelete(key: Sequence[str]) -> None: delete multiple keys
    yield_keys(prefix: Optional[str] = None) -> Iterator[str]: yield all keys in the store, optionally filtering by a prefix

For key-value store implementations, see this section.
Tools

Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models. Tools are needed whenever you want a model to control parts of your code or call out to external APIs.

A tool consists of:

    The name of the tool.
    A description of what the tool does.
    A JSON schema defining the inputs to the tool.
    A function (and, optionally, an async variant of the function).

When a tool is bound to a model, the name, description and JSON schema are provided as context to the model. Given a list of tools and a set of instructions, a model can request to call one or more tools with specific inputs. Typical usage may look like the following:

tools = [...] # Define a list of tools
llm_with_tools = llm.bind_tools(tools)
ai_msg = llm_with_tools.invoke("do xyz...")
# -> AIMessage(tool_calls=[ToolCall(...), ...], ...)

The AIMessage returned from the model MAY have tool_calls associated with it. Read this guide for more information on what the response type may look like.

Once the chosen tools are invoked, the results can be passed back to the model so that it can complete whatever task it's performing. There are generally two different ways to invoke the tool and pass back the response:
Invoke with just the arguments

When you invoke a tool with just the arguments, you will get back the raw tool output (usually a string). This generally looks like:

# You will want to previously check that the LLM returned tool calls
tool_call = ai_msg.tool_calls[0]
# ToolCall(args={...}, id=..., ...)
tool_output = tool.invoke(tool_call["args"])
tool_message = ToolMessage(
    content=tool_output,
    tool_call_id=tool_call["id"],
    name=tool_call["name"]
)

Note that the content field will generally be passed back to the model. If you do not want the raw tool response to be passed to the model, but you still want to keep it around, you can transform the tool output but also pass it as an artifact (read more about ToolMessage.artifact here)

... # Same code as above
response_for_llm = transform(response)
tool_message = ToolMessage(
    content=response_for_llm,
    tool_call_id=tool_call["id"],
    name=tool_call["name"],
    artifact=tool_output
)

Invoke with ToolCall

The other way to invoke a tool is to call it with the full ToolCall that was generated by the model. When you do this, the tool will return a ToolMessage. The benefits of this are that you don't have to write the logic yourself to transform the tool output into a ToolMessage. This generally looks like:

tool_call = ai_msg.tool_calls[0]
# -> ToolCall(args={...}, id=..., ...)
tool_message = tool.invoke(tool_call)
# -> ToolMessage(
#      content="tool result foobar...",
#      tool_call_id=...,
#      name="tool_name"
#    )

If you are invoking the tool this way and want to include an artifact for the ToolMessage, you will need to have the tool return two things. Read more about defining tools that return artifacts here.
Best practices

When designing tools to be used by a model, it is important to keep in mind that:

    Chat models that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models.
    Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas. This is another form of prompt engineering.
    Simple, narrowly scoped tools are easier for models to use than complex tools.

Related

For specifics on how to use tools, see the tools how-to guides.

To use a pre-built tool, see the tool integration docs.
Toolkits

Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.

All Toolkits expose a get_tools method which returns a list of tools. You can therefore do:

# Initialize a toolkit
toolkit = ExampleTookit(...)

# Get list of tools
tools = toolkit.get_tools()

Agents

By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating agents. Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be. The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.

LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. Please check out that documentation for a more in depth overview of agent concepts.

There is a legacy agent concept in LangChain that we are moving towards deprecating: AgentExecutor. AgentExecutor was essentially a runtime for agents. It was a great place to get started, however, it was not flexible enough as you started to have more customized agents. In order to solve that we built LangGraph to be this flexible, highly-controllable runtime.

If you are still using AgentExecutor, do not fear: we still have a guide on how to use AgentExecutor. It is recommended, however, that you start to transition to LangGraph. In order to assist in this, we have put together a transition guide on how to do so.
ReAct agents

One popular architecture for building agents is ReAct. ReAct combines reasoning and acting in an iterative process - in fact the name "ReAct" stands for "Reason" and "Act".

The general flow looks like this:

    The model will "think" about what step to take in response to an input and any previous observations.
    The model will then choose an action from available tools (or choose to respond to the user).
    The model will generate arguments to that tool.
    The agent runtime (executor) will parse out the chosen tool and call it with the generated arguments.
    The executor will return the results of the tool call back to the model as an observation.
    This process repeats until the agent chooses to respond.

There are general prompting based implementations that do not require any model-specific features, but the most reliable implementations use features like tool calling to reliably format outputs and reduce variance.

Please see the LangGraph documentation for more information, or this how-to guide for specific information on migrating to LangGraph.
Callbacks

LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.

You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.
Callback Events
Event	Event Trigger	Associated Method
Chat model start	When a chat model starts	on_chat_model_start
LLM start	When a llm starts	on_llm_start
LLM new token	When an llm OR chat model emits a new token	on_llm_new_token
LLM ends	When an llm OR chat model ends	on_llm_end
LLM errors	When an llm OR chat model errors	on_llm_error
Chain start	When a chain starts running	on_chain_start
Chain end	When a chain ends	on_chain_end
Chain error	When a chain errors	on_chain_error
Tool start	When a tool starts running	on_tool_start
Tool end	When a tool ends	on_tool_end
Tool error	When a tool errors	on_tool_error
Agent action	When an agent takes an action	on_agent_action
Agent finish	When an agent ends	on_agent_finish
Retriever start	When a retriever starts	on_retriever_start
Retriever end	When a retriever ends	on_retriever_end
Retriever error	When a retriever errors	on_retriever_error
Text	When arbitrary text is run	on_text
Retry	When a retry event is run	on_retry
Callback handlers

Callback handlers can either be sync or async:

    Sync callback handlers implement the BaseCallbackHandler interface.
    Async callback handlers implement the AsyncCallbackHandler interface.

During run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each "registered" callback handler when the event is triggered.
Passing callbacks

The callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:

    Request time callbacks: Passed at the time of the request in addition to the input data. Available on all standard Runnable objects. These callbacks are INHERITED by all children of the object they are defined on. For example, chain.invoke({"number": 25}, {"callbacks": [handler]}).
    Constructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks are passed as arguments to the constructor of the object. The callbacks are scoped only to the object they are defined on, and are not inherited by any children of the object.

warning

Constructor callbacks are scoped only to the object they are defined on. They are not inherited by children of the object.

If you're creating a custom chain or runnable, you need to remember to propagate request time callbacks to any child objects.
Async in Python<=3.10

Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables and is running async in python<=3.10, will have to propagate callbacks to child objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case.

This is a common reason why you may fail to see events being emitted from custom runnables or tools.

For specifics on how to use callbacks, see the relevant how-to guides here.
Techniques
Streaming

Individual LLM calls often run for much longer than traditional resource requests. This compounds when you build more complex chains or agents that require multiple reasoning steps.

Fortunately, LLMs generate output iteratively, which means it's possible to show sensible intermediate results before the final response is ready. Consuming output as soon as it becomes available has therefore become a vital part of the UX around building apps with LLMs to help alleviate latency issues, and LangChain aims to have first-class support for streaming.

Below, we'll discuss some concepts and considerations around streaming in LangChain.
.stream() and .astream()

Most modules in LangChain include the .stream() method (and the equivalent .astream() method for async environments) as an ergonomic streaming interface. .stream() returns an iterator, which you can consume with a simple for loop. Here's an example with a chat model:

from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

for chunk in model.stream("what color is the sky?"):
    print(chunk.content, end="|", flush=True)

API Reference:ChatAnthropic

For models (or other components) that don't support streaming natively, this iterator would just yield a single chunk, but you could still use the same general pattern when calling them. Using .stream() will also automatically call the model in streaming mode without the need to provide additional config.

The type of each outputted chunk depends on the type of component - for example, chat models yield AIMessageChunks. Because this method is part of LangChain Expression Language, you can handle formatting differences from different outputs using an output parser to transform each yielded chunk.

You can check out this guide for more detail on how to use .stream().
.astream_events()

While the .stream() method is intuitive, it can only return the final generated value of your chain. This is fine for single LLM calls, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output - for example, returning sources alongside the final generation when building a chat over documents app.

There are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate values to the end with something like chained .assign() calls, but LangChain also includes an .astream_events() method that combines the flexibility of callbacks with the ergonomics of .stream(). When called, it returns an iterator which yields various types of events that you can filter and process according to the needs of your project.

Here's one small example that prints just events containing streamed chat model output:

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}, version="v2"):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)

API Reference:StrOutputParser | ChatPromptTemplate | ChatAnthropic

You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!

See this guide for more detailed information on how to use .astream_events(), including a table listing available events.
Callbacks

The lowest level way to stream outputs from LLMs in LangChain is via the callbacks system. You can pass a callback handler that handles the on_llm_new_token event into LangChain components. When that component is invoked, any LLM or chat model contained in the component calls the callback with the generated token. Within the callback, you could pipe the tokens into some other destination, e.g. a HTTP response. You can also handle the on_llm_end event to perform any necessary cleanup.

You can see this how-to section for more specifics on using callbacks.

Callbacks were the first technique for streaming introduced in LangChain. While powerful and generalizable, they can be unwieldy for developers. For example:

    You need to explicitly initialize and manage some aggregator or other stream to collect results.
    The execution order isn't explicitly guaranteed, and you could theoretically have a callback run after the .invoke() method finishes.
    Providers would often make you pass an additional parameter to stream outputs instead of returning them all at once.
    You would often ignore the result of the actual model call in favor of callback results.

Tokens

The unit that most model providers use to measure input and output is via a unit called a token. Tokens are the basic units that language models read and generate when processing or producing text. The exact definition of a token can vary depending on the specific way the model was trained - for instance, in English, a token could be a single word like "apple", or a part of a word like "app".

When you send a model a prompt, the words and characters in the prompt are encoded into tokens using a tokenizer. The model then streams back generated output tokens, which the tokenizer decodes into human-readable text. The below example shows how OpenAI models tokenize LangChain is cool!:

You can see that it gets split into 5 different tokens, and that the boundaries between tokens are not exactly the same as word boundaries.

The reason language models use tokens rather than something more immediately intuitive like "characters" has to do with how they process and understand text. At a high-level, language models iteratively predict their next generated output based on the initial input and their previous generations. Training the model using tokens language models to handle linguistic units (like words or subwords) that carry meaning, rather than individual characters, which makes it easier for the model to learn and understand the structure of the language, including grammar and context. Furthermore, using tokens can also improve efficiency, since the model processes fewer units of text compared to character-level processing.
Function/tool calling
info

We use the term tool calling interchangeably with function calling. Although function calling is sometimes meant to refer to invocations of a single function, we treat all models as though they can return multiple tool or function calls in each message.

Tool calling allows a chat model to respond to a given prompt by generating output that matches a user-defined schema.

While the name implies that the model is performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user. One common example where you wouldn't want to call a function with the generated arguments is if you want to extract structured output matching some schema from unstructured text. You would give the model an "extraction" tool that takes parameters matching the desired schema, then treat the generated output as your final result.

Diagram of a tool call by a chat model

Tool calling is not universal, but is supported by many popular LLM providers, including Anthropic, Cohere, Google, Mistral, OpenAI, and even for locally-running models via Ollama.

LangChain provides a standardized interface for tool calling that is consistent across different models.

The standard interface consists of:

    ChatModel.bind_tools(): a method for specifying which tools are available for a model to call. This method accepts LangChain tools as well as Pydantic objects.
    AIMessage.tool_calls: an attribute on the AIMessage returned from the model for accessing the tool calls requested by the model.

Tool usage

After the model calls tools, you can use the tool by invoking it, then passing the arguments back to the model. LangChain provides the Tool abstraction to help you handle this.

The general flow is this:

    Generate tool calls with a chat model in response to a query.
    Invoke the appropriate tools using the generated tool call as arguments.
    Format the result of the tool invocations as ToolMessages.
    Pass the entire list of messages back to the model so that it can generate a final answer (or call more tools).

Diagram of a complete tool calling flow

This is how tool calling agents perform tasks and answer queries.

Check out some more focused guides below:

    How to use chat models to call tools
    How to pass tool outputs to chat models
    Building an agent with LangGraph

Structured output

LLMs are capable of generating arbitrary text. This enables the model to respond appropriately to a wide range of inputs, but for some use-cases, it can be useful to constrain the LLM's output to a specific format or structure. This is referred to as structured output.

For example, if the output is to be stored in a relational database, it is much easier if the model generates output that adheres to a defined schema or format. Extracting specific information from unstructured text is another case where this is particularly useful. Most commonly, the output format will be JSON, though other formats such as YAML can be useful too. Below, we'll discuss a few ways to get structured output from models in LangChain.
.with_structured_output()

For convenience, some LangChain chat models support a .with_structured_output() method. This method only requires a schema as input, and returns a dict or Pydantic object. Generally, this method is only present on models that support one of the more advanced methods described below, and will use one of them under the hood. It takes care of importing a suitable output parser and formatting the schema in the right format for the model.

Here's an example:

from typing import Optional

from pydantic import BaseModel, Field


class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")

structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")

Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)

We recommend this method as a starting point when working with structured output:

    It uses other model-specific features under the hood, without the need to import an output parser.
    For the models that use tool calling, no special prompting is needed.
    If multiple underlying techniques are supported, you can supply a method parameter to toggle which one is used.

You may want or need to use other techniques if:

    The chat model you are using does not support tool calling.
    You are working with very complex schemas and the model is having trouble generating outputs that conform.

For more information, check out this how-to guide.

You can also check out this table for a list of models that support with_structured_output().
Raw prompting

The most intuitive way to get a model to structure output is to ask nicely. In addition to your query, you can give instructions describing what kind of output you'd like, then parse the output using an output parser to convert the raw model message or string output into something more easily manipulated.

The biggest benefit to raw prompting is its flexibility:

    Raw prompting does not require any special model features, only sufficient reasoning capability to understand the passed schema.
    You can prompt for any format you'd like, not just JSON. This can be useful if the model you are using is more heavily trained on a certain type of data, such as XML or YAML.

However, there are some drawbacks too:

    LLMs are non-deterministic, and prompting a LLM to consistently output data in the exactly correct format for smooth parsing can be surprisingly difficult and model-specific.
    Individual models have quirks depending on the data they were trained on, and optimizing prompts can be quite difficult. Some may be better at interpreting JSON schema, others may be best with TypeScript definitions, and still others may prefer XML.

While features offered by model providers may increase reliability, prompting techniques remain important for tuning your results no matter which method you choose.
JSON mode

Some models, such as Mistral, OpenAI, Together AI and Ollama, support a feature called JSON mode, usually enabled via config.

When enabled, JSON mode will constrain the model's output to always be some sort of valid JSON. Often they require some custom prompting, but it's usually much less burdensome than completely raw prompting and more along the lines of, "you must always return JSON". The output also generally easier to parse.

It's also generally simpler to use directly and more commonly available than tool calling, and can give more flexibility around prompting and shaping results than tool calling.

Here's an example:

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers.json import SimpleJsonOutputParser

model = ChatOpenAI(
    model="gpt-4o",
    model_kwargs={ "response_format": { "type": "json_object" } },
)

prompt = ChatPromptTemplate.from_template(
    "Answer the user's question to the best of your ability."
    'You must always output a JSON object with an "answer" key and a "followup_question" key.'
    "{question}"
)

chain = prompt | model | SimpleJsonOutputParser()

chain.invoke({ "question": "What is the powerhouse of the cell?" })

API Reference:ChatPromptTemplate | ChatOpenAI | SimpleJsonOutputParser

{'answer': 'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.',
 'followup_question': 'Would you like to know more about how mitochondria produce energy?'}

For a full list of model providers that support JSON mode, see this table.
Tool calling

For models that support it, tool calling can be very convenient for structured output. It removes the guesswork around how best to prompt schemas in favor of a built-in model feature.

It works by first binding the desired schema either directly or via a LangChain tool to a chat model using the .bind_tools() method. The model will then generate an AIMessage containing a tool_calls field containing args that match the desired shape.

There are several acceptable formats you can use to bind tools to a model in LangChain. Here's one example:

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI

class ResponseFormatter(BaseModel):
    """Always use this tool to structure your response to the user."""

    answer: str = Field(description="The answer to the user's question")
    followup_question: str = Field(description="A followup question the user could ask")

model = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
)

model_with_tools = model.bind_tools([ResponseFormatter])

ai_msg = model_with_tools.invoke("What is the powerhouse of the cell?")

ai_msg.tool_calls[0]["args"]

API Reference:ChatOpenAI

{'answer': "The powerhouse of the cell is the mitochondrion. It generates most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.",
 'followup_question': 'How do mitochondria generate ATP?'}

Tool calling is a generally consistent way to get a model to generate structured output, and is the default technique used for the .with_structured_output() method when a model supports it.

The following how-to guides are good practical resources for using function/tool calling for structured output:

    How to return structured data from an LLM
    How to use a model to call tools

For a full list of model providers that support tool calling, see this table.
Few-shot prompting

One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as "few-shot prompting". The technique is based on the Language Models are Few-Shot Learners paper. There are a few things to think about when doing few-shot prompting:

    How are examples generated?
    How many examples are in each prompt?
    How are examples selected at runtime?
    How are examples formatted in the prompt?

Here are the considerations for each.
1. Generating examples

The first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.

At a high-level, the basic ways to generate examples are:

    Manual: a person/people generates examples they think are useful.
    Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.
    User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).
    LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.

Which approach is best depends on your task. For tasks where a small number core principles need to be understood really well, it can be valuable hand-craft a few really good examples. For tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input.

Single-turn v.s. multi-turn examples

Another dimension to think about when generating examples is what the example is actually showing.

The simplest types of examples just have a user input and an expected model output. These are single-turn examples.

One more complex type if example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer. This is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where its useful to show common errors and spell out exactly why they're wrong and what should be done instead.
2. Number of examples

Once we have a dataset of examples, we need to think about how many examples should be in each prompt. The key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency. And beyond some threshold having too many examples can start to confuse the model. Finding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints. Anecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples. But, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.
3. Selecting examples

Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:

    Randomly
    By (semantic or keyword-based) similarity of the inputs
    Based on some other constraints, like token size

LangChain has a number of ExampleSelectors which make it easy to use any of these techniques.

Generally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.
4. Formatting examples

Most state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples:

    In the system prompt as a string
    As their own messages

If we insert our examples into the system prompt as a string, we'll need to make sure it's clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like ChatML, XML, TypeScript, etc.

If we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign names to our messages like "example_user" and "example_assistant" to make it clear that these messages correspond to different actors than the latest input message.

Formatting tool call examples

One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.

    Some models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,
    Some models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,
    Some models require that tools are passed in to the model if there are any tool calls / ToolMessages in the chat history.

These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints. In these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.

You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks here.
Retrieval

LLMs are trained on a large but fixed dataset, limiting their ability to reason over private or recent information. Fine-tuning an LLM with specific facts is one way to mitigate this, but is often poorly suited for factual recall and can be costly. Retrieval is the process of providing relevant information to an LLM to improve its response for a given input. Retrieval augmented generation (RAG) paper is the process of grounding the LLM generation (output) using the retrieved information.
tip

    See our RAG from Scratch code and video series.
    For a high-level guide on retrieval, see this tutorial on RAG.

RAG is only as good as the retrieved documentsâ€™ relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems. We've focused on taxonomizing and summarizing many of these techniques (see below figure) and will share some high-level strategic guidance in the following sections. You can and should experiment with using different pieces together. You might also find this LangSmith guide useful for showing how to evaluate different iterations of your app.

Query Translation

First, consider the user input(s) to your RAG system. Ideally, a RAG system can handle a wide range of inputs, from poorly worded questions to complex multi-part queries. Using an LLM to review and optionally modify the input is the central idea behind query translation. This serves as a general buffer, optimizing raw user inputs for your retrieval system. For example, this can be as simple as extracting keywords or as complex as generating multiple sub-questions for a complex query.
Name	When to use	Description
Multi-query	When you need to cover multiple perspectives of a question.	Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, return the unique documents for all queries.
Decomposition	When a question can be broken down into smaller subproblems.	Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).
Step-back	When a higher-level conceptual understanding is required.	First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. Paper.
HyDE	If you have challenges retrieving relevant documents using the raw user inputs.	Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. Paper.
tip

See our RAG from Scratch videos for a few different specific approaches:

    Multi-query
    Decomposition
    Step-back
    HyDE

Routing

Second, consider the data sources available to your RAG system. You want to query across more than one database or across structured and unstructured data sources. Using an LLM to review the input and route it to the appropriate data source is a simple and effective approach for querying across sources.
Name	When to use	Description
Logical routing	When you can prompt an LLM with rules to decide where to route the input.	Logical routing can use an LLM to reason about the query and choose which datastore is most appropriate.
Semantic routing	When semantic similarity is an effective way to determine where to route the input.	Semantic routing embeds both query and, typically a set of prompts. It then chooses the appropriate prompt based upon similarity.
tip

See our RAG from Scratch video on routing.
Query Construction

Third, consider whether any of your data sources require specific query formats. Many structured databases use SQL. Vector stores often have specific syntax for applying keyword filters to document metadata. Using an LLM to convert a natural language query into a query syntax is a popular and powerful approach. In particular, text-to-SQL, text-to-Cypher, and query analysis for metadata filters are useful ways to interact with structured, graph, and vector databases respectively.
Name	When to Use	Description
Text to SQL	If users are asking questions that require information housed in a relational database, accessible via SQL.	This uses an LLM to transform user input into a SQL query.
Text-to-Cypher	If users are asking questions that require information housed in a graph database, accessible via Cypher.	This uses an LLM to transform user input into a Cypher query.
Self Query	If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.	This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).
tip

See our blog post overview and RAG from Scratch video on query construction, the process of text-to-DSL where DSL is a domain specific language required to interact with a given database. This converts user questions into structured queries.
Indexing

Fourth, consider the design of your document index. A simple and powerful idea is to decouple the documents that you index for retrieval from the documents that you pass to the LLM for generation. Indexing frequently uses embedding models with vector stores, which compress the semantic information in documents to fixed-size vectors.

Many RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question. Furthermore, LLMs are increasingly capable of processing millions of tokens.

Two approaches can address this tension: (1) Multi Vector retriever using an LLM to translate documents into any form (e.g., often into a summary) that is well-suited for indexing, but returns full documents to the LLM for generation. (2) ParentDocument retriever embeds document chunks, but also returns full documents. The idea is to get the best of both worlds: use concise representations (summaries or chunks) for retrieval, but use the full documents for answer generation.
Name	Index Type	Uses an LLM	When to Use	Description
Vector store	Vector store	No	If you are just getting started and looking for something quick and easy.	This is the simplest method and the one that is easiest to get started with. It involves creating embeddings for each piece of text.
ParentDocument	Vector store + Document Store	No	If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.	This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).
Multi Vector	Vector store + Document Store	Sometimes during indexing	If you are able to extract information from documents that you think is more relevant to index than the text itself.	This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.
Time-Weighted Vector store	Vector store	No	If you have timestamps associated with your documents, and you want to retrieve the most recent ones	This fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents)
tip

    See our RAG from Scratch video on indexing fundamentals
    See our RAG from Scratch video on multi vector retriever

Fifth, consider ways to improve the quality of your similarity search itself. Embedding models compress text into fixed-length (vector) representations that capture the semantic content of the document. This compression is useful for search / retrieval, but puts a heavy burden on that single vector representation to capture the semantic nuance / detail of the document. In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding.

ColBERT is an interesting approach to address this with a higher granularity embeddings: (1) produce a contextually influenced embedding for each token in the document and query, (2) score similarity between each query token and all document tokens, (3) take the max, (4) do this for all query tokens, and (5) take the sum of the max scores (in step 3) for all query tokens to get a query-document similarity score; this token-wise scoring can yield strong results.

There are some additional tricks to improve the quality of your retrieval. Embeddings excel at capturing semantic information, but may struggle with keyword-based queries. Many vector stores offer built-in hybrid-search to combine keyword and semantic similarity, which marries the benefits of both approaches. Furthermore, many vector stores have maximal marginal relevance, which attempts to diversify the results of a search to avoid returning similar and redundant documents.
Name	When to use	Description
ColBERT	When higher granularity embeddings are needed.	ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score. Paper.
Hybrid search	When combining keyword-based and semantic similarity.	Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. Paper.
Maximal Marginal Relevance (MMR)	When needing to diversify search results.	MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.
tip

See our RAG from Scratch video on ColBERT.
Post-processing

Sixth, consider ways to filter or rank retrieved documents. This is very useful if you are combining documents returned from multiple sources, since it can can down-rank less relevant documents and / or compress similar documents.
Name	Index Type	Uses an LLM	When to Use	Description
Contextual Compression	Any	Sometimes	If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.	This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.
Ensemble	Any	No	If you have multiple retrieval methods and want to try combining them.	This fetches documents from multiple retrievers and then combines them.
Re-ranking	Any	Yes	If you want to rank retrieved documents based upon relevance, especially if you want to combine results from multiple retrieval methods .	Given a query and a list of documents, Rerank indexes the documents from most to least semantically relevant to the query.
tip

See our RAG from Scratch video on RAG-Fusion (paper), on approach for post-processing across multiple queries: Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, and combine the ranks of multiple search result lists to produce a single, unified ranking with Reciprocal Rank Fusion (RRF).
Generation

Finally, consider ways to build self-correction into your RAG system. RAG systems can suffer from low quality retrieval (e.g., if a user question is out of the domain for the index) and / or hallucinations in generation. A naive retrieve-generate pipeline has no ability to detect or self-correct from these kinds of errors. The concept of "flow engineering" has been introduced in the context of code generation: iteratively build an answer to a code question with unit tests to check and self-correct errors. Several works have applied this RAG, such as Self-RAG and Corrective-RAG. In both cases, checks for document relevance, hallucinations, and / or answer quality are performed in the RAG answer generation flow.

We've found that graphs are a great way to reliably express logical flows and have implemented ideas from several of these papers using LangGraph, as shown in the figure below (red - routing, blue - fallback, green - self-correction):

    Routing: Adaptive RAG (paper). Route questions to different retrieval approaches, as discussed above
    Fallback: Corrective RAG (paper). Fallback to web search if docs are not relevant to query
    Self-correction: Self-RAG (paper). Fix answers w/ hallucinations or donâ€™t address question

Name	When to use	Description
Self-RAG	When needing to fix answers with hallucinations or irrelevant content.	Self-RAG performs checks for document relevance, hallucinations, and answer quality during the RAG answer generation flow, iteratively building an answer and self-correcting errors.
Corrective-RAG	When needing a fallback mechanism for low relevance docs.	Corrective-RAG includes a fallback (e.g., to web search) if the retrieved documents are not relevant to the query, ensuring higher quality and more relevant retrieval.
tip

See several videos and cookbooks showcasing RAG with LangGraph:

    LangGraph Corrective RAG
    LangGraph combining Adaptive, Self-RAG, and Corrective RAG
    Cookbooks for RAG using LangGraph

See our LangGraph RAG recipes with partners:

    Meta
    Mistral

Text splitting

LangChain offers many different types of text splitters. These all live in the langchain-text-splitters package.

Table columns:

    Name: Name of the text splitter
    Classes: Classes that implement this text splitter
    Splits On: How this text splitter splits text
    Adds Metadata: Whether or not this text splitter adds metadata about where each chunk came from.
    Description: Description of the splitter, including recommendation on when to use it.

Name	Classes	Splits On	Adds Metadata	Description
Recursive	RecursiveCharacterTextSplitter, RecursiveJsonSplitter	A list of user defined characters		Recursively splits text. This splitting is trying to keep related pieces of text next to each other. This is the recommended way to start splitting text.
HTML	HTMLHeaderTextSplitter, HTMLSectionSplitter	HTML specific characters	âœ…	Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML)
Markdown	MarkdownHeaderTextSplitter,	Markdown specific characters	âœ…	Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)
Code	many languages	Code (Python, JS) specific characters		Splits text based on characters specific to coding languages. 15 different languages are available to choose from.
Token	many classes	Tokens		Splits text on tokens. There exist a few different ways to measure tokens.
Character	CharacterTextSplitter	A user defined character		Splits text based on a user defined character. One of the simpler methods.
Semantic Chunker (Experimental)	SemanticChunker	Sentences		First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg Kamradt
Integration: AI21 Semantic	AI21SemanticTextSplitter		âœ…	Identifies distinct topics that form coherent pieces of text and splits along those.
Evaluation

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications. It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.

LangSmith helps with this process in a few ways:

    It makes it easier to create and curate datasets via its tracing and annotation features
    It provides an evaluation framework that helps you define metrics and run your app against your dataset
    It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code

To learn more, check out this LangSmith guide.
Tracing

A trace is essentially a series of steps that your application takes to go from input to output. Traces contain individual steps called runs. These can be individual calls from a model, retriever, tool, or sub-chains. Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

For a deeper dive, check out this LangSmith conceptual guide.